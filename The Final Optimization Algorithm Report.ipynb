{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf5cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Optimazation algoriths on digits Dataset\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "digits=load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25605d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "for i in range(2):\n",
    "    plt.matshow(digits.images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd21ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizing present available sets\n",
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3739af16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6461a8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b88416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.DESCR[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc41c582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7207e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and training a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1514d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084807db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e597aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "#Creating and training a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df430979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9711111111111111"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measuring Model Accuracy with the test=25% and train_size=75%\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88499081",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.4,train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d076f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "#Creating and training a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "452e5982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624478442280946"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measuring Model Accuracy with the test=40% and train_size=60%\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae8cd5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting the set of data upto the last digit set\n",
    "model.predict(digits.data[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "683b3fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 9, 6, 7, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(digits.data[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29575168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a confusion Matrix\n",
    "#It actually shows that the elements have been correctly predicted\n",
    "y_predict=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebfb436b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70,  0,  0,  0,  2,  1,  2,  0,  0,  0],\n",
       "       [ 0, 74,  0,  0,  0,  0,  0,  0,  2,  1],\n",
       "       [ 0,  1, 65,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 66,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0, 72,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  1,  0, 63,  0,  1,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0, 69,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0,  0,  0,  0, 74,  1,  0],\n",
       "       [ 0,  3,  0,  0,  1,  0,  0,  1, 73,  0],\n",
       "       [ 0,  0,  0,  1,  0,  1,  0,  0,  1, 66]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d80c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "             #Liblinear\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from grad_utils import grad_logloss_theta_lr\n",
    "from grad_utils import batch_grad_logloss_lr\n",
    "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
    "from dataset import load_digits,select_from_one_class\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd81cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset used\n",
    "dataset_name = \"load_digits\"\n",
    "# dataset_name = \"load_digits\"\n",
    "\n",
    "# parameter for the sigmoid sampling function\n",
    "sigmoid_k = 10\n",
    "# regularization parameter for Logistic Regression\n",
    "C = 0.1\n",
    "# sample ratio\n",
    "sample_ratio = 0.6\n",
    "# flip ratio\n",
    "flip_ratio = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1211061",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# load data, pick 30% as the Va set\n",
    "x_train,y_train,x_va,y_va,x_te,y_te = load_data_v1(dataset_name,va_ratio=0.3)\n",
    "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
    "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
    "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
    "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
    "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
    "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))\n",
    "print(\"Load data, cost {:.1f} sec\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76360b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9622222222222222"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        #Computing score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "digits=load_digits()\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model=LogisticRegression()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,train_size=0.75)\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)\n",
    "#Measuring Model Accuracy\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subset samples number\n",
    "num_tr_sample = x_train.shape[0]\n",
    "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
    "\n",
    "\n",
    "# flip labels\n",
    "idxs = np.arange(y_train.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "num_flip = int(flip_ratio * len(idxs))\n",
    "y_train[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train[idxs[:num_flip]]).astype(int)\n",
    "\n",
    "\n",
    "# define the full-set-model \\hat{\\theta}\n",
    "clf = LogisticRegression(\n",
    "        C = C,\n",
    "        fit_intercept=False,\n",
    "        tol = 1e-8,\n",
    "        solver=\"liblinear\",\n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=100,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        )\n",
    "clf.fit(x_train,y_train)\n",
    "# on Va\n",
    "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
    "full_logloss = log_loss(y_va,y_va_pred)\n",
    "weight_ar = clf.coef_.flatten()\n",
    "# on Te\n",
    "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
    "full_te_logloss = log_loss(y_te,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_te, y_te_pred)\n",
    "y_te_pred = clf.predict(x_te)\n",
    "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "# print full-set-model results\n",
    "print(\"[FullSet] Va logloss {:.6f}\".format(full_logloss))\n",
    "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efaa360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing AUC\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "dataset_name = \"load_digits\"\n",
    "a = tf.Variable([0.1, 0.5])\n",
    "b = tf.Variable([0.2, 0.6])\n",
    "\n",
    "auc = tf.contrib.metrics.streaming_auc(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.initialize_local_variables()) \n",
    "train_auc = sess.run(auc)\n",
    "\n",
    "print(train_auc)\n",
    "#The auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7005b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0, 39,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2, 48,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 37,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0, 43,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  1,  3,  0, 44,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 48,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  1,  0, 40,  0,  1],\n",
       "       [ 0,  1,  1,  0,  0,  1,  0,  0, 47,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  0,  0, 36]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the Confusion Matrix\n",
    "y_predict=model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "#from grad_utils import grad_logloss_theta_lr\n",
    "#from grad_utils import batch_grad_logloss_lr\n",
    "#from inverse_hvp import inverse_hvp_lr_newtonCG\n",
    "#from dataset import load_digits,select_from_one_class\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b79949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset used\n",
    "dataset_name = \"load_digits\"\n",
    "# dataset_name = \"load_digits\"\n",
    "\n",
    "# parameter for the sigmoid sampling function\n",
    "sigmoid_k = 10\n",
    "# regularization parameter for Logistic Regression\n",
    "C = 0.1\n",
    "# sample ratio\n",
    "sample_ratio = 0.6\n",
    "# flip ratio\n",
    "flip_ratio = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# load data, pick 75% as the Va set\n",
    "x_train,y_train,x_va,y_va,x_te,y_te = load_data_v1(dataset_name,va_ratio=0.75)\n",
    "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
    "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
    "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
    "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
    "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
    "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))\n",
    "print(\"Load data, cost {:.1f} sec\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f580ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        #Computing score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "digits=load_digits()\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model=LogisticRegression()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,train_size=0.75)\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)\n",
    "#Measuring Model Accuracy\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subset samples number\n",
    "num_tr_sample = x_train.shape[0]\n",
    "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
    "\n",
    "\n",
    "# flip labels\n",
    "idxs = np.arange(y_train.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "num_flip = int(flip_ratio * len(idxs))\n",
    "y_train[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train[idxs[:num_flip]]).astype(int)\n",
    "\n",
    "\n",
    "# define the full-set-model \\hat{\\theta}\n",
    "clf = LogisticRegression(\n",
    "        C = C,\n",
    "        fit_intercept=False,\n",
    "        tol = 1e-8,\n",
    "        solver=\"liblinear\",\n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=100,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        )\n",
    "clf.fit(x_train,y_train)\n",
    "# on Va\n",
    "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
    "full_logloss = log_loss(y_va,y_va_pred)\n",
    "weight_ar = clf.coef_.flatten()\n",
    "# on Te\n",
    "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
    "full_te_logloss = log_loss(y_te,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_te, y_te_pred)\n",
    "y_te_pred = clf.predict(x_te)\n",
    "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "# print full-set-model results\n",
    "print(\"[FullSet] Va logloss {:.6f}\".format(full_logloss))\n",
    "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing AUC\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "dataset_name = \"load_digits\"\n",
    "a = tf.Variable([0.1, 0.5])\n",
    "b = tf.Variable([0.2, 0.6])\n",
    "\n",
    "auc = tf.contrib.metrics.streaming_auc(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.initialize_local_variables()) \n",
    "train_auc = sess.run(auc)\n",
    "\n",
    "print(train_auc)\n",
    "#The auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b46957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 0, 52,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 42,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1, 30,  0,  0,  0,  0,  2,  1],\n",
       "       [ 0,  0,  0,  0, 47,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0, 42,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 51,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 42,  0,  1],\n",
       "       [ 0,  2,  1,  0,  0,  0,  0,  0, 34,  0],\n",
       "       [ 1,  0,  0,  0,  0,  1,  0,  0,  1, 48]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the Confusion Matrix\n",
    "y_predict=model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_start_time = time.time()\n",
    "# building precoditioner\n",
    "test_grad_loss_val = grad_logloss_theta_lr(y_va,y_va_pred,x_va,weight_ar,C,False,0.1/(num_tr_sample*C))\n",
    "tr_pred = clf.predict_proba(x_train)[:,1]\n",
    "batch_size = 10000\n",
    "M = None\n",
    "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
    "for idx in range(total_batch):\n",
    "    batch_tr_grad = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        weight_ar,\n",
    "        C,\n",
    "        False,\n",
    "        1.0)\n",
    "\n",
    "    sum_grad = batch_tr_grad.multiply(x_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
    "    if M is None:\n",
    "        M = sum_grad\n",
    "    else:\n",
    "        M = M + sum_grad\n",
    "        \n",
    "M = M + 0.1/(num_tr_sample*C) * np.ones(x_train.shape[1])\n",
    "M = np.array(M).flatten()\n",
    "# computing the inverse Hessian-vector-product\n",
    "iv_hvp = inverse_hvp_lr_newtonCG(x_train,y_train,tr_pred,test_grad_loss_val,C,True,1e-5,True,M,0.1/(num_tr_sample*C))\n",
    "# get influence score\n",
    "total_batch = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
    "predicted_loss_diff = []\n",
    "for idx in range(total_batch):\n",
    "    train_grad_loss_val = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        weight_ar,\n",
    "           C,\n",
    "        False,\n",
    "        1.0)\n",
    "\n",
    "    predicted_loss_diff.extend(np.array(train_grad_loss_val.dot(iv_hvp)).flatten())\n",
    "    \n",
    "predicted_loss_diffs = np.asarray(predicted_loss_diff)\n",
    "duration = time.time() - if_start_time\n",
    "print(\"The Influence function's computation completed, cost {:.1f} sec\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==\"*75)\n",
    "print(\"IF Stats: mean {:.10f}, max {:.10f}, min {:.10f}\".format(\n",
    "    predicted_loss_diffs.mean(), predicted_loss_diffs.max(), predicted_loss_diffs.min())\n",
    ")\n",
    "# building sampling probability\n",
    "phi_ar = - predicted_loss_diffs\n",
    "IF_interval = phi_ar.max() - phi_ar.min()\n",
    "a_param = sigmoid_k / IF_interval\n",
    "prob_pi = 1 / (1 + np.exp(a_param * phi_ar))\n",
    "print(\"Pi Stats:\",np.percentile(prob_pi,[10,25,50,75,90]))\n",
    "\n",
    "# Do subsampling\n",
    "pos_idx = select_from_one_class(y_train,prob_pi,1,sample_ratio)\n",
    "neg_idx = select_from_one_class(y_train,prob_pi,0,sample_ratio)\n",
    "sb_idx = np.union1d(pos_idx,neg_idx)\n",
    "sb_x_train = x_train[sb_idx]\n",
    "sb_y_train = y_train[sb_idx]\n",
    "\n",
    "# Train the subset-model \\tilde{\\theta}\n",
    "clf.fit(sb_x_train,sb_y_train)\n",
    "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
    "sb_logloss = log_loss(y_va, y_va_pred)\n",
    "sb_weight = clf.coef_.flatten()\n",
    "diff_w_norm = np.linalg.norm(weight_ar - sb_weight)\n",
    "sb_size = sb_x_train.shape[0]\n",
    "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
    "sb_te_logloss = log_loss(y_te,y_te_pred)\n",
    "sb_te_auc = roc_auc_score(y_te, y_te_pred)\n",
    "y_te_pred = clf.predict(x_te)\n",
    "sb_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# baseline: random sampling\n",
    "u_idxs = np.arange(x_train.shape[0])\n",
    "uniform_idxs = np.random.choice(u_idxs,obj_sample_size,replace=False)\n",
    "us_x_train = x_train[uniform_idxs]\n",
    "us_y_train = y_train[uniform_idxs]\n",
    "clf.fit(us_x_train, us_y_train)\n",
    "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
    "us_logloss = log_loss(y_va, y_va_pred)\n",
    "us_size = us_x_train.shape[0]\n",
    "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
    "us_te_logloss = log_loss(y_te,y_te_pred)\n",
    "us_te_auc = roc_auc_score(y_te, y_te_pred)\n",
    "y_te_pred = clf.predict(x_te)\n",
    "us_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "print(\"==\"*75)\n",
    "print(\"Result Summary on Va\")\n",
    "print(\"[SigUIDS]  logloss {:.6f}, # {}\".format(sb_logloss,sb_size))\n",
    "print(\"[Random]   logloss {:.6f}, # {}\".format(us_logloss,us_size))\n",
    "print(\"[Full]     logloss {:.6f}, # {}\".format(full_logloss,num_tr_sample))\n",
    "print(\"Result Summary on Te\")\n",
    "print(\"[SigUIDS]  logloss {:.6f}, # {}\".format(sb_te_logloss,sb_size))\n",
    "print(\"[Random]   logloss {:.6f}, # {}\".format(us_te_logloss,us_size))\n",
    "print(\"[Full]     logloss {:.6f}, # {}\".format(full_te_logloss,num_tr_sample))\n",
    "print(\"==\"*30)\n",
    "# Attention: if the dataset used here is small, one experiment may fail because of uncertainty of subsampling!\n",
    "# besides, a proper $k$ for sigmoid sampling function is also important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31615ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton-cg\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from grad_hinge import batch_grad_hinge_loss, grad_hinge_loss_theta, hessian_hingle_loss_theta\n",
    "from grad_hinge import inverse_hvp_hinge_newtonCG\n",
    "\n",
    "from dataset import load_data_v1,select_from_one_class\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now selecting the dataset used\n",
    "dataset_name = \"load_digits\"\n",
    "# parameter for the sigmoid sampling function\n",
    "sigmoid_k = 10\n",
    "# sample ratio\n",
    "sample_ratio = 0.6\n",
    "flip_ratio = 0.4\n",
    "\n",
    "acc_func = lambda x,y: (x==y).sum() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86269a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# load data, pick 60% as the Va set\n",
    "x_train,y_train,x_va,y_va,x_te,y_te = load_data_v1(dataset_name,va_ratio=0.6)\n",
    "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
    "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
    "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
    "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
    "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
    "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))\n",
    "print(\"Load data, cost {:.1f} sec\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfa7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "      #Computing score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "digits=load_digits()\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model=LogisticRegression()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,train_size=0.75)\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)\n",
    "#Measuring Model Accuracy\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tr_sample = x_train.shape[0]\n",
    "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
    "\n",
    "# define the full-set-model \\hat{\\theta}\n",
    "clf = LinearSVC(loss=\"squared_hinge\", dual=False, fit_intercept=False)\n",
    "\n",
    "# flip labels\n",
    "idxs = np.arange(y_train.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "num_flip = int(flip_ratio * len(idxs))\n",
    "y_train[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train[idxs[:num_flip]]).astype(int)\n",
    "                                                                                              \n",
    "# zero to -1\n",
    "y_train[y_train == 0] = -1\n",
    "y_va[y_va == 0] = -1\n",
    "y_te[y_te == 0] = -1\n",
    "\n",
    "\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ac767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing AUC\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "dataset_name = \"load_digits\"\n",
    "a = tf.Variable([0.1, 0.5])\n",
    "b = tf.Variable([0.2, 0.6])\n",
    "\n",
    "auc = tf.contrib.metrics.streaming_auc(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.initialize_local_variables()) \n",
    "train_auc = sess.run(auc)\n",
    "\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_va_pred = clf.predict(x_va)\n",
    "full_acc = acc_func(y_va, y_va_pred)\n",
    "weight_ar = clf.coef_.flatten()\n",
    "# on Te\n",
    "y_te_pred = clf.predict(x_te)\n",
    "full_te_acc = acc_func(y_te,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_te,y_te_pred)\n",
    "# print full-set-model results\n",
    "print(\"[FullSet] Va acc {:.6f}\".format(full_acc))\n",
    "print(\"[FullSet] Te acc {:.6f}\".format(full_te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function value: -0.08498508673969181\n",
    "Split function value: 0.0, -0.08498508673969181\n",
    "Optimization terminated successfully.\n",
    "         Current function value: -0.084985\n",
    "         Iterations: 1\n",
    "        Function evaluations: 2\n",
    "        Gradient evaluations: 2\n",
    "        Hessian evaluations: 1\n",
    "implicit hessian-vector products mean: -0.07872199256311764\n",
    "implicit hessian-vector products norm: 0.29152201758991003\n",
    "Inverse HVP took 0.0 sec\n",
    "The Influence function's computation completed, cost 0.0 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728937ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0, 49,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0, 43,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 38,  0,  0,  0,  0,  1,  1],\n",
       "       [ 0,  1,  0,  0, 42,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 46,  0,  1,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0, 46,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 44,  0,  1],\n",
       "       [ 0,  3,  0,  0,  0,  0,  0,  0, 42,  0],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0,  0,  1, 41]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the Confusion Matrix\n",
    "y_predict=model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lbfgs \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import cifar10 # to load dataset\n",
    "\n",
    "from utils import compute_stats, get_grad\n",
    "from LBFGS import FullBatchLBFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for L-BFGS training\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
    "\n",
    "# Define network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20764ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cuda availability\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Create neural network model\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(2018)\n",
    "    model = ConvNet().cuda()\n",
    "else:\n",
    "    torch.manual_seed(2018)\n",
    "    model = ConvNet()\n",
    "\n",
    "# Define helper functions\n",
    "\n",
    "# Forward pass\n",
    "if cuda:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
    "else:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
    "\n",
    "# Forward pass through the network given the input\n",
    "if cuda:\n",
    "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
    "else:\n",
    "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f831083c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FullBatchLBFGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m accfun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m op, y: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mequal(predsfun(op), y\u001b[38;5;241m.\u001b[39msqueeze())) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mFullBatchLBFGS\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, history_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, line_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWolfe\u001b[39m\u001b[38;5;124m'\u001b[39m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Main training loop\u001b[39;00m\n\u001b[0;32m      8\u001b[0m no_samples \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FullBatchLBFGS' is not defined"
     ]
    }
   ],
   "source": [
    "# Do the forward pass, then compute the accuracy\n",
    "accfun = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze())) * 100\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = FullBatchLBFGS(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n",
    "\n",
    "# Main training loop\n",
    "no_samples = X_train.shape[0]\n",
    "\n",
    "# compute initial gradient and objective\n",
    "grad, obj = get_grad(optimizer, X_train, y_train, opfun)\n",
    "\n",
    "# main loop\n",
    "for n_iter in range(max_iter):\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "\n",
    "    # define closure for line search\n",
    "    def closure():\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if cuda:\n",
    "            loss_fn = torch.tensor(0, dtype=torch.float).cuda()\n",
    "        else:\n",
    "            loss_fn = torch.tensor(0, dtype=torch.float)\n",
    "\n",
    "        for subsmpl in np.array_split(np.arange(no_samples), max(int(no_samples / ghost_batch), 1)):\n",
    "\n",
    "            ops = opfun(X_train[subsmpl])\n",
    "\n",
    "            if cuda:\n",
    "                tgts = torch.from_numpy(y_train[subsmpl]).cuda().long().squeeze()\n",
    "            else:\n",
    "                tgts = torch.from_numpy(y_train[subsmpl]).long().squeeze()\n",
    "\n",
    "            loss_fn += F.cross_entropy(ops, tgts) * (len(subsmpl) / no_samples)\n",
    "\n",
    "        return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449da3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   #Computing score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "digits=load_digits()\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model=LogisticRegression()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target,test_size=0.25,train_size=0.75)\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "model.fit(x_train,y_train)\n",
    "#Measuring Model Accuracy\n",
    "#Getting the model Score\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e1212",
   "metadata": {},
   "outputs": [],
   "source": [
    " # perform line search step\n",
    "    options = {'closure': closure, 'current_loss': obj}\n",
    "    obj, grad, lr, _, _, _, _, _ = optimizer.step(options)\n",
    "\n",
    "    # compute statistics\n",
    "    model.eval()\n",
    "    train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n",
    "                                                    ghost_batch=128)\n",
    "\n",
    "    # print data\n",
    "    print('Iter:', n_iter + 1, 'lr:', lr, 'Training Loss:', train_loss, 'Test Loss:', test_loss,\n",
    "          'Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing AUC\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import tensorflow as tf\n",
    "dataset_name = \"load_digits\"\n",
    "a = tf.Variable([0.1, 0.5])\n",
    "b = tf.Variable([0.2, 0.6])\n",
    "\n",
    "auc = tf.contrib.metrics.streaming_auc(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.initialize_local_variables()) \n",
    "train_auc = sess.run(auc)\n",
    "\n",
    "print(train_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9df63e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# compute statistics\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m train_loss, test_loss, test_acc \u001b[38;5;241m=\u001b[39m compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n\u001b[0;32m      4\u001b[0m                                                     ghost_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# print data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# compute statistics\n",
    "model.eval()\n",
    "train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n",
    "                                                    ghost_batch=128)\n",
    "\n",
    "    # print data\n",
    "print('Iter:', n_iter + 1, 'lr:', lr, 'Training Loss:', train_loss, 'Test Loss:', test_loss,\n",
    "          'Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0163cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Calculating the Confusion Matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_predict\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      4\u001b[0m cm\u001b[38;5;241m=\u001b[39mconfusion_matrix(y_test,y_predict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Calculating the Confusion Matrix\n",
    "y_predict=model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c59533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The end of optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
