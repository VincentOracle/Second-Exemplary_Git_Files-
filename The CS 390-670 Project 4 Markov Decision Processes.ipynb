{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299eddb4",
   "metadata": {},
   "source": [
    "# CS 390-670 Project 4 Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98acc72",
   "metadata": {},
   "source": [
    "# MDP 1: Weather Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0049d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration results:\n",
      "State values: [850.68483805 795.8903175 ]\n",
      "Optimal policy: [0 1]\n",
      "Policy iteration results:\n",
      "Optimal policy: [0 1]\n",
      "State values: [850.68483805 795.8903175 ]\n"
     ]
    }
   ],
   "source": [
    "# MDP 1: Weather Forecasting\n",
    "# This MDP models a weather forecasting problem. The agent has to decide whether to issue a forecast of \n",
    "# \"rain\" or \"no rain\" based on the current state of the weather. The weather can either be \"sunny\" or \"cloudy\".\n",
    "# The agent receives a reward of +10 for issuing an accurate forecast and a reward of -10 for issuing an inaccurate forecast.\n",
    "\n",
    "# States: sunny, cloudy\n",
    "# Actions: rain, no rain\n",
    "# Rewards: +10 for accurate forecast, -10 for inaccurate forecast\n",
    "# Transition probabilities:\n",
    "\n",
    "# If the weather is sunny and the agent issues \"rain\", there is a 30% chance of rain and a 70% chance of no rain.\n",
    "# If the weather is sunny and the agent issues \"no rain\", there is a 70% chance of no rain and a 30% chance of rain.\n",
    "# If the weather is cloudy and the agent issues \"rain\", there is a 60% chance of rain and a 40% chance of no rain.\n",
    "# If the weather is cloudy and the agent issues \"no rain\", there is a 40% chance of no rain and a 60% chance of rain.\n",
    "\n",
    "\n",
    "#IMPLEMENTATION\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the MDP\n",
    "P = np.array([\n",
    "    # action 0: sunny\n",
    "    [[0.9, 0.1], [0.2, 0.8]],\n",
    "\n",
    "    # action 1: rainy\n",
    "    [[0.3, 0.7], [0.6, 0.4]]\n",
    "])\n",
    "\n",
    "R = np.array([\n",
    "    # action 0: sunny\n",
    "    [100, 0],\n",
    "\n",
    "    # action 1: rainy\n",
    "    [50, 50]\n",
    "])\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the functions for value iteration and policy iteration\n",
    "def value_iteration(P, R, gamma, theta=1e-5):\n",
    "    V = np.zeros(P.shape[0])\n",
    "    while True:\n",
    "        Q = np.sum(P * (R + gamma * V), axis=2)\n",
    "        V_new = np.max(Q, axis=1)\n",
    "        if np.max(np.abs(V - V_new)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    return V\n",
    "\n",
    "def policy_iteration(P, R, gamma, theta=1e-5):\n",
    "    n_states, n_actions, _ = P.shape\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    while True:\n",
    "        V = value_iteration(P, R, gamma, theta=theta)\n",
    "        Q = np.sum(P * (R + gamma * V), axis=2)\n",
    "        policy_new = np.argmax(Q, axis=1)\n",
    "        if np.array_equal(policy, policy_new):\n",
    "            break\n",
    "        policy = policy_new\n",
    "    return policy\n",
    "\n",
    "# Solve the MDP using value iteration and policy iteration\n",
    "print(\"Value iteration results:\")\n",
    "V = value_iteration(P, R, gamma)\n",
    "print(\"State values:\", V)\n",
    "policy = policy_iteration(P, R, gamma)\n",
    "print(\"Optimal policy:\", policy)\n",
    "\n",
    "print(\"Policy iteration results:\")\n",
    "policy = policy_iteration(P, R, gamma)\n",
    "print(\"Optimal policy:\", policy)\n",
    "V = value_iteration(P, R, gamma)\n",
    "print(\"State values:\", V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd464bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5 0.5 0. ]\n",
      "  [0.  0.5 0.5]\n",
      "  [0.  0.  1. ]]\n",
      "\n",
      " [[0.5 0.  0.5]\n",
      "  [0.5 0.  0.5]\n",
      "  [0.  0.  1. ]]\n",
      "\n",
      " [[0.  0.5 0.5]\n",
      "  [0.5 0.  0.5]\n",
      "  [0.  0.  1. ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox\n",
    "\n",
    "# Define the MDP parameters\n",
    "P = np.array([[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.0, 1.0]],\n",
    "              [[0.5, 0.0, 0.5], [0.5, 0.0, 0.5], [0.0, 0.0, 1.0]],\n",
    "              [[0.0, 0.5, 0.5], [0.5, 0.0, 0.5], [0.0, 0.0, 1.0]]])\n",
    "R = np.array([[10, 0, 0], [0, 5, 0], [0, 0, 1]])\n",
    "discount = 0.9\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6072bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  5  0]\n",
      " [ 0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Policy:  [0 1 2]\n",
      "Policy Iteration Optimal Value Function:  [0. 0. 0.]\n",
      "Policy Iteration Number of Iterations:  0\n"
     ]
    }
   ],
   "source": [
    "# Create an MDP object\n",
    "mdp = mdptoolbox.mdp.PolicyIteration(P, R, discount)\n",
    "\n",
    "# Solve the MDP using policy iteration\n",
    "pi_policy = mdp.policy\n",
    "pi_V = mdp.V\n",
    "pi_num_iter = mdp.iter\n",
    "\n",
    "print(\"Policy Iteration Policy: \", pi_policy)\n",
    "print(\"Policy Iteration Optimal Value Function: \", pi_V)\n",
    "print(\"Policy Iteration Number of Iterations: \", pi_num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71648f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7. 2. 6. 6. 3. 7. 7. 1. 5. 1. 4. 9. 5.]\n",
      " [6. 2. 9. 9. 7. 8. 8. 6. 6. 1. 4. 7. 5.]\n",
      " [6. 1. 3. 3. 3. 6. 7. 1. 4. 1. 5. 5. 7.]\n",
      " [8. 2. 5. 4. 3. 7. 8. 0. 5. 1. 4. 9. 7.]\n",
      " [8. 2. 5. 4. 4. 6. 8. 4. 6. 1. 5. 5. 6.]\n",
      " [5. 1. 6. 5. 2. 7. 6. 0. 4. 1. 4. 8. 7.]\n",
      " [6. 1. 4. 4. 2. 6. 6. 0. 3. 1. 3. 5. 9.]\n",
      " [7. 2. 6. 5. 5. 6. 7. 1. 4. 2. 4. 5. 7.]\n",
      " [6. 1. 6. 5. 3. 7. 7. 1. 4. 2. 4. 4. 8.]\n",
      " [6. 2. 7. 4. 4. 9. 9. 1. 5. 3. 4. 5. 9.]\n",
      " [0. 1. 4. 7. 1. 7. 6. 7. 6. 0. 9. 5. 0.]\n",
      " [4. 0. 0. 0. 1. 4. 0. 1. 0. 0. 4. 2. 1.]\n",
      " [3. 0. 6. 9. 0. 4. 4. 4. 4. 0. 6. 5. 2.]\n",
      " [3. 0. 3. 2. 9. 4. 5. 2. 9. 1. 4. 3. 4.]\n",
      " [5. 5. 3. 3. 0. 2. 2. 5. 1. 1. 1. 3. 0.]\n",
      " [4. 0. 6. 5. 3. 6. 7. 3. 7. 2. 5. 3. 3.]\n",
      " [6. 0. 1. 3. 1. 4. 5. 0. 4. 2. 5. 4. 0.]\n",
      " [4. 0. 3. 6. 0. 5. 5. 0. 2. 2. 4. 8. 1.]\n",
      " [6. 6. 5. 8. 1. 2. 4. 9. 5. 2. 2. 2. 1.]\n",
      " [6. 0. 1. 3. 0. 9. 9. 0. 7. 3. 4. 6. 1.]\n",
      " [3. 8. 6. 7. 1. 1. 0. 7. 1. 1. 1. 0. 3.]\n",
      " [5. 3. 5. 9. 3. 0. 2. 0. 1. 3. 0. 0. 2.]\n",
      " [7. 6. 4. 6. 1. 2. 0. 9. 1. 3. 2. 2. 2.]\n",
      " [8. 5. 6. 6. 5. 3. 0. 7. 1. 3. 3. 2. 3.]\n",
      " [7. 7. 6. 8. 4. 3. 1. 5. 4. 4. 1. 1. 4.]\n",
      " [6. 9. 5. 7. 2. 3. 0. 9. 4. 5. 0. 0. 3.]\n",
      " [3. 6. 4. 5. 5. 1. 1. 1. 3. 5. 0. 2. 5.]\n",
      " [7. 3. 7. 9. 4. 2. 1. 4. 4. 6. 1. 1. 4.]\n",
      " [6. 3. 5. 6. 7. 2. 0. 7. 4. 6. 0. 1. 5.]\n",
      " [9. 1. 7. 9. 3. 8. 3. 7. 9. 9. 0. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocess the data by normalizing and discretizing\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\n\\\\Downloads\\\\wines.csv\")\n",
    "scaler = MinMaxScaler()\n",
    "data_norm = scaler.fit_transform(data.iloc[:,1:])\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "data_disc = est.fit_transform(data_norm)\n",
    "print(data_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e446db",
   "metadata": {},
   "source": [
    "# MDP 2: Robot Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92051fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Description:\n",
    "# Imagine a robot that is navigating in a grid world. The robot can move up, down, left, or right, \n",
    "# and the objective is to reach the goal state while avoiding obstacles. The robot receives a reward of \n",
    "# +10 for reaching the goal state and a reward of -1 for hitting an obstacle. The robot receives a reward of -0.1 for\n",
    "# each time step it spends in the grid world. The grid world has a finite size, and the robot cannot move outside of \n",
    "# the grid world.\n",
    "\n",
    "# MDP Definition:\n",
    "# States: The states of the MDP are represented by the robot's location on the grid world.\n",
    "# Actions: The actions of the MDP are represented by the robot's movement in the grid world (up, down, left, right).\n",
    "# Rewards: The rewards of the MDP are defined as follows:\n",
    "# +10 for reaching the goal state.\n",
    "# -1 for hitting an obstacle.\n",
    "# -0.1 for each time step spent in the grid world.\n",
    "# Transition Probabilities: The transition probabilities for each action depend on the robot's \n",
    "#     current location and the action taken. If the robot attempts to move into an obstacle or outside the grid world, \n",
    "#     it remains in its current location with probability 1.\n",
    "\n",
    "#IMPLEMENTATION \n",
    "\n",
    "import numpy as np\n",
    "class RobotNavigationMDP:\n",
    "    def __init__(self, width, height, start_state, end_state, obstacle_states, step_cost=-1, end_reward=10):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.states = [(x, y) for x in range(width) for y in range(height)]\n",
    "        self.start_state = start_state\n",
    "        self.end_state = end_state\n",
    "        self.obstacle_states = obstacle_states\n",
    "        self.step_cost = step_cost\n",
    "        self.end_reward = end_reward\n",
    "\n",
    "    def actions(self, state):\n",
    "        if state == self.end_state:\n",
    "            return [None]\n",
    "        if state in self.obstacle_states:\n",
    "            return [None]\n",
    "        actions = [\"north\", \"south\", \"east\", \"west\"]\n",
    "        x, y = state\n",
    "        if x == 0:\n",
    "            actions.remove(\"west\")\n",
    "        elif x == self.width - 1:\n",
    "            actions.remove(\"east\")\n",
    "        if y == 0:\n",
    "            actions.remove(\"north\")\n",
    "        elif y == self.height - 1:\n",
    "            actions.remove(\"south\")\n",
    "        return actions\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        if next_state == self.end_state:\n",
    "            return self.end_reward\n",
    "        if next_state in self.obstacle_states:\n",
    "            return 0\n",
    "        return self.step_cost\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        if action is None:\n",
    "            return [(1.0, state)]\n",
    "        next_states = {}\n",
    "        for a in self.actions(state):\n",
    "            if a == action:\n",
    "                prob = 0.7\n",
    "            else:\n",
    "                prob = 0.1\n",
    "            next_state = self._move(state, a)\n",
    "            if next_state not in next_states:\n",
    "                next_states[next_state] = 0\n",
    "            next_states[next_state] += prob\n",
    "        return [(prob, next_state) for next_state, prob in next_states.items()]\n",
    "\n",
    "    def _move(self, state, action):\n",
    "        x, y = state\n",
    "        if action == \"north\":\n",
    "            return (x, y - 1)\n",
    "        elif action == \"south\":\n",
    "            return (x, y + 1)\n",
    "        elif action == \"east\":\n",
    "            return (x + 1, y)\n",
    "        elif action == \"west\":\n",
    "            return (x - 1, y)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def value_iteration(self, gamma, theta):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                V[s] = max(sum(p * (self.reward(s, a, next_s) + gamma * V[next_s]) for (p, next_s) in self.transition(s, a)) for a in self.actions(s))\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        policy = {s: max(self.actions(s), key=lambda a: sum(p * (self.reward(s, a, next_s) + gamma * V[next_s]) for (p, next_s) in self.transition(s, a))) for s in self.states}\n",
    "        return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77793283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[850.68483805 795.8903175 ]\n"
     ]
    }
   ],
   "source": [
    "print(policy)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(self, gamma, theta):\n",
    "    V = {s: 0 for s in self.states}\n",
    "    policy = {s: self.actions(s)[0] for s in self.states}\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                a = policy[s]\n",
    "                V[s] = sum([self.transition_probs[s][a][next_s] * (self.rewards[s][a][next_s] + gamma * V[next_s]) \n",
    "                            for next_s in self.states])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        policy_stable = True\n",
    "        for s in self.states:\n",
    "            old_action = policy[s]\n",
    "            policy[s] = self.actions(s)[np.argmax([sum([self.transition_probs[s][a][next_s] * \n",
    "                                                      (self.rewards[s][a][next_s] + gamma * V[next_s]) \n",
    "                                                      for next_s in self.states]) \n",
    "                                                      for a in self.actions(s)])]\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the Delta\n",
    "print( delta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3830fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d13fc",
   "metadata": {},
   "source": [
    "# Taxi MDP using the gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c261bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Set up MDP parameters\n",
    "gamma = 0.9\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "theta = 0.0001\n",
    "\n",
    "# Value iteration algorithm\n",
    "V = [0] * num_states\n",
    "delta = float('inf')\n",
    "iter_count = 0\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s in range(num_states):\n",
    "        v = V[s]\n",
    "        max_q = float('-inf')\n",
    "        for a in range(num_actions):\n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q += prob * (reward + gamma * V[next_state])\n",
    "            max_q = max(max_q, q)\n",
    "        V[s] = max_q\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    iter_count += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Value iteration converged in\", iter_count, \"iterations\")\n",
    "print(\"V =\", V)\n",
    "\n",
    "# Policy iteration algorithm\n",
    "V = [0] * num_states\n",
    "pi = [0] * num_states\n",
    "stable_policy = False\n",
    "iter_count = 0\n",
    "while not stable_policy:\n",
    "    # Policy evaluation\n",
    "    delta = float('inf')\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            v = V[s]\n",
    "            a = pi[s]\n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q += prob * (reward + gamma * V[next_state])\n",
    "            V[s] = q\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "    # Policy improvement\n",
    "    stable_policy = True\n",
    "    for s in range(num_states):\n",
    "        old_action = pi[s]\n",
    "        max_q = float('-inf')\n",
    "        for a in range(num_actions):\n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q += prob * (reward + gamma * V[next_state])\n",
    "            if q > max_q:\n",
    "                max_q = q\n",
    "                pi[s] = a\n",
    "        if pi[s] != old_action:\n",
    "            stable_policy = False\n",
    "\n",
    "    iter_count += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Policy iteration converged in\", iter_count, \"iterations\")\n",
    "print(\"V =\", V)\n",
    "print(\"pi =\", pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba191f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_prob(state, action, possible_state):\n",
    "    \"\"\"\n",
    "    Get the probability of transitioning from state to possible_state given the action taken.\n",
    "    Args:\n",
    "        state (tuple): the current state\n",
    "        action (int): the index of the action taken\n",
    "        possible_state (tuple): the possible new state resulting from taking the action\n",
    "    Returns:\n",
    "        float: the probability of transitioning to possible_state given state and action\n",
    "    \"\"\"\n",
    "    # implement the transition function\n",
    "    # this could be a deterministic or stochastic function, depending on the MDP\n",
    "    # for example, it could depend on the Taxi , and color\n",
    "    # and involve a series of if-else statements or a probability distribution\n",
    "    return 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71086f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the states\n",
    "import itertools\n",
    "Taxi_levels = [\"low\", \"medium\", \"high\"]\n",
    "color_levels = [\"low\", \"medium\", \"high\"]\n",
    "proline_levels = [\"low\", \"medium\", \"high\"]\n",
    "states = list(itertools.product(Taxi_levels, color_levels, proline_levels))\n",
    "\n",
    "# define the state space\n",
    "state_space = {}\n",
    "for i, state in enumerate(states):\n",
    "    state_space[state] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get new state function\n",
    "def get_new_state(state, action):\n",
    "    Taxi , color, proline = state\n",
    "    if action == 0: # low action\n",
    "        if Taxi  == \"low\":\n",
    "            new_Taxi  = \"low\"\n",
    "        else:\n",
    "            new_alcohol = \"low\" if np.random.random() < 0.8 else \"medium\"\n",
    "    elif action == 1: # medium action\n",
    "        if Taxi  == \"low\":\n",
    "            new_Taxi  = \"medium\"\n",
    "        elif Taxi  == \"medium\":\n",
    "            new_Taxi  = \"medium\" if np.random.random() < 0.5 else \"high\"\n",
    "        else:\n",
    "            new_Taxi  = \"high\" if np.random.random() < 0.8 else \"medium\"\n",
    "    else: # high action\n",
    "        if Taxi  == \"high\":\n",
    "            new_Taxi  = \"high\"\n",
    "        else:\n",
    "            new_Taxi  = \"high\" if np.random.random() < 0.8 else \"medium\"\n",
    "    return (new_Taxi , color, proline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get wine ratings function\n",
    "def get_Taxi_rating(state):\n",
    "    \"\"\"\n",
    "    Given a state, returns a rating for the quality of the wine, which is determined by the alcohol content and\n",
    "    the color intensity of the wine.\n",
    "    \"\"\"\n",
    "    Taxi , color, proline = state\n",
    "    if Taxi  == \"low\":\n",
    "        if color == \"low\":\n",
    "            return 1\n",
    "        elif color == \"medium\":\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    elif Taxi  == \"medium\":\n",
    "        if color == \"low\":\n",
    "            return 2\n",
    "        elif color == \"medium\":\n",
    "            return 4\n",
    "        else:\n",
    "            return 6\n",
    "    else:\n",
    "        if color == \"low\":\n",
    "            return 3\n",
    "        elif color == \"medium\":\n",
    "            return 6\n",
    "        else:\n",
    "            return 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rewards\n",
    "actions = [\"low\", \"medium\", \"high\"]\n",
    "rewards = np.zeros((len(actions), len(states)))\n",
    "for i, state in enumerate(states):\n",
    "    for j, action in enumerate(actions):\n",
    "        # get the new state after taking the action\n",
    "        new_state = get_new_state(state, action)\n",
    "        # calculate the reward based on the quality rating of the wine\n",
    "        rating = get_Taxi_rating(new_state)\n",
    "        reward = rating * 10\n",
    "        rewards[j, i] = reward\n",
    "print(\"Reward is\",\" \" ,rewards[j, i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0cf90",
   "metadata": {},
   "source": [
    "#    THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
